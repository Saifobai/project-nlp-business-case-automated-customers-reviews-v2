# -*- coding: utf-8 -*-
"""AutoReviewModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rNrqO8eJIP3t_DbZ-VBCD_7CG40BjQFo

##### ============================================================
##### 🎬 IMDB Movie Reviews Analysis Pipeline (Functional)
##### Tasks:
##### 1) Sentiment Classification (3 classes)
##### 2) Movie Clustering (4-6 categories)
##### 3) Review Summarization (per movie or cluster)
##### ============================================================
"""

!pip install -q transformers datasets evaluate accelerate peft mlflow

# =============================================
# 1) Imports
# =============================================
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import torch

from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, pipeline
)
from peft import LoraConfig, get_peft_model, PeftModel
from transformers import DataCollatorWithPadding, EarlyStoppingCallback
import evaluate
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from openai import OpenAI
from textblob import TextBlob

# MLflow
import mlflow
import mlflow.sklearn
import mlflow.transformers
from mlflow import log_metric, log_param, log_artifact

# Set MLflow experiment
mlflow.set_experiment("Movie_Review_Analysis")

from google.colab import userdata
openai_api_key = userdata.get('OPENAI_API_KEY')


os.environ["OPENAI_API_KEY"] = openai_api_key

# =============================================
# 2) Sentiment Model Creation
# =============================================
def create_sentiment_model(model_name="distilbert-base-uncased", num_labels=3):
    """
    Create a transformer sentiment classification model with LoRA adapters.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
    model.config.id2label = {0: "NEGATIVE", 1: "POSITIVE", 2: "NEUTRAL"}
    model.config.label2id = {"NEGATIVE": 0, "POSITIVE": 1, "NEUTRAL": 2}

    # LoRA adapters
    lora_config = LoraConfig(
        r=16, lora_alpha=32, lora_dropout=0.1,
        bias="none", task_type="SEQ_CLS",
        target_modules=["q_lin", "k_lin", "v_lin"]
    )
    model = get_peft_model(model, lora_config)

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    accuracy_metric = evaluate.load("accuracy")
    f1_metric = evaluate.load("f1")

    return model, tokenizer, data_collator, accuracy_metric, f1_metric


def compute_metrics(eval_pred, accuracy_metric, f1_metric):
    """
    Compute accuracy and F1 during evaluation.
    """
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_metric.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1_metric.compute(predictions=preds, references=labels, average="weighted")["f1"]
    }


def train_sentiment(
    model,
    tokenizer,
    data_collator,
    accuracy_metric,
    f1_metric,
    train_dataset,
    eval_dataset,
    output_dir="./results",
    epochs=5
):
    """
    Train the sentiment model on IMDB dataset.
    """
    # Tokenize datasets
    train_enc = train_dataset.map(
        lambda x: tokenizer(x["text"], truncation=True, max_length=256),
        batched=True,
        remove_columns=["text"]
    )
    eval_enc = eval_dataset.map(
        lambda x: tokenizer(x["text"], truncation=True, max_length=256),
        batched=True,
        remove_columns=["text"]
    )
    train_enc.set_format("torch")
    eval_enc.set_format("torch")

    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=2e-4,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=epochs,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=100,
        fp16=True,
        gradient_accumulation_steps=2,
        load_best_model_at_end=True
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_enc,
        eval_dataset=eval_enc,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=lambda p: compute_metrics(p, accuracy_metric, f1_metric),
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    )

    trainer.train()
    return trainer, eval_enc


def predict_sentiment(trainer_or_model, tokenizer, texts):
    """
    Run inference for sentiment analysis.
    """
    pipe = pipeline("text-classification",
                    model=trainer_or_model.model if hasattr(trainer_or_model, 'model') else trainer_or_model,
                    tokenizer=tokenizer)
    return pipe(texts)

# =============================================
# 3) Movie Clustering with Transformer Embeddings
# =============================================
def build_text_representation(row):
    """
    Build a text string from movie metadata to be embedded.
    """
    return f"{row['Movie_Title']} | {row['main_genre']} {row['side_genre']} | {row['Director']} | {row['Actors']}"


def prepare_clustering(movies_csv, n_clusters=5):
    """
    Create clustering model using transformer embeddings.
    """
    df = pd.read_csv(movies_csv)

    # Build text representation
    df["text_repr"] = df.apply(build_text_representation, axis=1)

    # Encode with SentenceTransformer
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = embedder.encode(df["text_repr"].tolist(), show_progress_bar=True)

    # Cluster embeddings
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    df["Cluster"] = kmeans.fit_predict(embeddings)

    return df, embedder, kmeans


def predict_cluster(movie, embedder, kmeans):
    """
    Predict cluster for a new movie.
    """
    text = f"{movie['movie_title']} | {movie['main_genre']} {movie['side_genre']} | {movie['Director']} | {movie['Actors']}"
    embedding = embedder.encode([text])
    return kmeans.predict(embedding)[0]

# =============================================
# 4) Summarization (GPT, same style as your Flask app)
# =============================================
# Create a single OpenAI client (reads API key from env: OPENAI_API_KEY)
_openai_client = None
def _get_openai_client():
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    return _openai_client

def gpt_summarize_reviews(reviews, model="gpt-4o-mini", max_tokens=300, temperature=0.7):
    """
    Summarize a list of reviews using GPT (matching your Flask logic),
    with a TextBlob-based fallback if the API call fails.
    """
    text = "\n".join(reviews or [])
    if not text.strip():
        return ""

    try:
        client = _get_openai_client()
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that summarizes movie reviews."},
                {"role": "user", "content": f"Summarize the following movie reviews in 3 to 5 sentences. Capture the overall sentiment, key strengths/weaknesses, and any notable themes:\n\n{text}"},
            ],
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print("❌ GPT summarization failed, falling back:", e)
        # Simple fallback: use TextBlob sentence split
        try:
            blob = TextBlob(text)
            sentences = [str(s) for s in blob.sentences]
            if len(sentences) > 3:
                return " ".join(sentences[:3])
            return text or "No summary available."
        except Exception:
            return text[:500]  # ultra-safe fallback

# =============================================
# 5) Training + Saving with MLflow
# =============================================
# Load IMDB dataset
dataset = load_dataset("imdb")

def map_to_three_classes(example):
    # IMDB is binary: 0=NEG, 1=POS. We randomly tag ~10% as NEUTRAL (label=2).
    if np.random.rand() < 0.1:
        return {"labels": 2, "text": example["text"]}
    return {"labels": example["label"], "text": example["text"]}

train_dataset = dataset["train"].map(map_to_three_classes)
test_dataset  = dataset["test"].map(map_to_three_classes)

with mlflow.start_run(run_name="Sentiment_Training"):
    # Train sentiment model
    model, tokenizer, data_collator, acc_metric, f1_metric = create_sentiment_model()
    trainer, eval_enc = train_sentiment(
        model, tokenizer, data_collator, acc_metric, f1_metric,
        train_dataset, test_dataset
    )

    # Evaluate
    eval_results = trainer.evaluate(eval_enc)
    print("📊 Sentiment Eval:", eval_results)

    mlflow.log_metrics(eval_results)

    # Save model
    save_path = "./sentiment_model"
    if isinstance(model, PeftModel):
        print("⚡ Detected adapter model → merging...")
        merged_model = model.merge_and_unload()
        merged_model.save_pretrained(save_path)
        to_log_model = merged_model
    else:
        model.save_pretrained(save_path)
        to_log_model = model
    tokenizer.save_pretrained(save_path)

    print(f"✅ Sentiment model saved to {save_path}")

    # Log to MLflow
    mlflow.transformers.log_model(
        transformers_model={"model": to_log_model, "tokenizer": tokenizer},
        artifact_path="sentiment_model"
    )

    # Save clustering artifacts
    movies_csv = "./imdb_data.csv"  # <-- Make sure this CSV exists alongside the notebook
    df_movies, embedder, kmeans = prepare_clustering(movies_csv, n_clusters=5)
    joblib.dump((embedder, kmeans), "clustering.pkl")
    mlflow.log_artifact("clustering.pkl")

    print("✅ Clustering artifacts saved & logged")

    # Confusion Matrix
    preds_output = trainer.predict(eval_enc)
    y_true = preds_output.label_ids
    y_pred = preds_output.predictions.argmax(axis=-1)
    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["NEGATIVE", "POSITIVE", "NEUTRAL"])
    plt.figure(figsize=(6,5))
    disp.plot(cmap="Blues", values_format="d", ax=plt.gca(), colorbar=False)
    plt.title("Confusion Matrix")
    plt.savefig("confusion_matrix.png")
    mlflow.log_artifact("confusion_matrix.png")
    plt.close()

# =============================================
# 6) Reload + Test
# =============================================
sentiment_model = AutoModelForSequenceClassification.from_pretrained("./sentiment_model", num_labels=3)
sentiment_pipe = pipeline("text-classification", model=sentiment_model, tokenizer=tokenizer)

print("Reloaded labels:", sentiment_model.config.id2label)

embedder, kmeans = joblib.load("clustering.pkl")

print(sentiment_pipe("Loved the movie, amazing acting!"))
print(sentiment_pipe("Terrible, I hated it."))

new_movie = {
    "movie_title": "Dune Part Two (2025)",
    "main_genre": "Action",
    "side_genre": "Sci-Fi",
    "Director": "Denis Villeneuve",
    "Actors": "Timothée Chalamet, Zendaya"
}
print(f"{new_movie['movie_title']} → Cluster {predict_cluster(new_movie, embedder, kmeans)}")

sample_reviews = [
    "Amazing movie with great visuals!",
    "Story was slow but acting was good.",
    "Loved the soundtrack and cinematography."
]
print("Summary (GPT):", gpt_summarize_reviews(sample_reviews))

# =============================================
# 7) Visualization / Plots
# =============================================
# Extract logs
training_logs = trainer.state.log_history
train_loss = [log["loss"] for log in training_logs if "loss" in log]
eval_loss = [log["eval_loss"] for log in training_logs if "eval_loss" in log]
eval_accuracy = [log["eval_accuracy"] for log in training_logs if "eval_accuracy" in log]
eval_f1 = [log["eval_f1"] for log in training_logs if "eval_f1" in log]
epochs = range(1, len(eval_loss) + 1)

# --- Loss Plot ---
plt.figure(figsize=(7, 5))
plt.plot(range(1, len(train_loss) + 1), train_loss, label="Training Loss", color="tab:blue", marker="o")
if eval_loss:
    plt.plot(epochs, eval_loss, label="Validation Loss", color="tab:orange", marker="s")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig("training_loss.png")
mlflow.log_artifact("training_loss.png")
plt.show()

# --- Accuracy & F1 Plot ---
plt.figure(figsize=(7, 5))
if eval_accuracy:
    plt.plot(epochs, eval_accuracy, label="Validation Accuracy", color="tab:green", marker="o")
if eval_f1:
    plt.plot(epochs, eval_f1, label="Validation F1", color="tab:red", marker="s")
plt.xlabel("Epochs")
plt.ylabel("Score")
plt.title("Validation Accuracy & F1")
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig("validation_metrics.png")
mlflow.log_artifact("validation_metrics.png")
plt.show()

# --- Confusion Matrix (Counts) ---
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["NEGATIVE", "POSITIVE", "NEUTRAL"])
fig, ax = plt.subplots(figsize=(6, 5))
disp.plot(cmap="Blues", values_format="d", ax=ax, colorbar=False)
plt.title("Confusion Matrix")
plt.tight_layout()
plt.savefig("confusion_matrix.png")
mlflow.log_artifact("confusion_matrix.png")
plt.show()

# --- Confusion Matrix (Normalized %) ---
cm_normalized = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]
plt.figure(figsize=(6, 5))
sns.heatmap(
    cm_normalized, annot=True, fmt=".2f", cmap="Blues",
    xticklabels=["NEGATIVE", "POSITIVE", "NEUTRAL"],
    yticklabels=["NEGATIVE", "POSITIVE", "NEUTRAL"],
    cbar_kws={"label": "Proportion"}
)
plt.ylabel("True Label")
plt.xlabel("Predicted Label")
plt.title("Confusion Matrix (Normalized %)")
plt.tight_layout()
plt.savefig("confusion_matrix_normalized.png")
mlflow.log_artifact("confusion_matrix_normalized.png")
plt.show()

# --- Cluster Distribution ---
plt.figure(figsize=(7, 5))
sns.countplot(x=df_movies["Cluster"], palette="viridis", edgecolor="black")
plt.xlabel("Cluster ID")
plt.ylabel("Number of Movies")
plt.title("Movie Distribution by Cluster")
plt.tight_layout()
plt.savefig("clustering_distribution.png")
mlflow.log_artifact("clustering_distribution.png")
plt.show()

# =============================================
#  Download current mlruns (to keep your results)
# =============================================
!zip -r mlruns.zip mlruns
from google.colab import files
files.download("mlruns.zip")

print("✅ mlruns.zip downloaded — unzip it into your local project folder to inspect with MLflow.")


# =============================================
# ☁️ 2) Setup persistent MLflow logging on Google Drive
# =============================================
from google.colab import drive
import mlflow

# Mount Google Drive
drive.mount("/content/drive")

# Define a folder in Drive for MLflow logs
mlflow_dir = "/content/drive/MyDrive/mlflow_runs"

# Tell MLflow to use that folder
mlflow.set_tracking_uri(f"file:{mlflow_dir}")

print(f"✅ MLflow is now tracking to: {mlflow_dir}")
print("ℹ️ Future runs will be saved here, even if Colab restarts.")

# # =============================================
# # 6) MLflow Logging (Models + Metrics + Artifacts)
# # =============================================
# import mlflow
# import mlflow.sklearn
# import mlflow.transformers

# mlflow.set_tracking_uri(f"file:{mlflow_dir}")   # already defined earlier
# mlflow.set_experiment("AutoReview-Experiment")

# with mlflow.start_run() as run:
#     # ✅ Log evaluation metrics
#     mlflow.log_metrics({
#         "eval_accuracy": eval_results["eval_accuracy"],
#         "eval_f1": eval_results["eval_f1"],
#         "eval_loss": eval_results["eval_loss"]
#     })

#     # ✅ Log hyperparameters
#     mlflow.log_params({
#         "epochs": 5,
#         "learning_rate": 2e-4,
#         "batch_size": 16,
#         "model_name": "distilbert-base-uncased",
#         "lora": True
#     })

#     # ✅ Log Hugging Face sentiment model
#     mlflow.transformers.log_model(
#         transformers_model={
#             "model": trainer.model,
#             "tokenizer": tokenizer
#         },
#         artifact_path="sentiment_model",
#         registered_model_name="SentimentClassifier"
#     )

#     # ✅ Log clustering artifacts (SentenceTransformer + KMeans)
#     mlflow.sklearn.log_model(kmeans, "kmeans_cluster_model")
#     joblib.dump(embedder, "embedder.pkl")
#     mlflow.log_artifact("embedder.pkl")

#     # ✅ Log figures
#     plt.savefig("training_curves.png")
#     mlflow.log_artifact("training_curves.png")

#     # ✅ Log confusion matrix
#     plt.savefig("confusion_matrix.png")
#     mlflow.log_artifact("confusion_matrix.png")

# print("🎉 Logged everything into MLflow!")
# print("Run the MLflow UI with:\nmlflow ui --backend-store-uri file:/content/drive/MyDrive/mlflow_runs")